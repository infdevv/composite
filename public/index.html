<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>compositeâ´</title>
    <script src="https://cdn.socket.io/4.8.1/socket.io.min.js" defer></script>
    <script src="./plugin-parse.js" defer></script>
    <script src="./hyper.js" defer></script>
    <style>
        body {
            background-color: #1a1a1a;
            color: white;
            font-family: 'Trebuchet MS', sans-serif;
            align-items: center;
            justify-content: center;
            text-align: center;
            padding: 8px;
        }

        .code {
            font-family: monospace;
            background-color: #272727;
            padding: 4px;
            border-radius: 4px;
            ;
        }

        button {
            background-color: #484948;
            border: none;
            color: white;
            padding: 7px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            font-size: 16px;
            margin: 4px 2px;
            cursor: pointer;
            border-radius: 4px;
        }

        #debug {
            display: none;
        }

        #logs {
            background-color: #272727;
            color: white;
            font-family: monospace;
            text-align: left;
            margin-left: calc(50% - calc(70% / 2));
            padding: 4px;
            height: 300px;
            width: 70%;
            overflow-y: scroll;
            border-radius: 4px;
        }

        select {
            background-color: #484948;
            border: none;
            color: white;
            padding: 7px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            font-size: 16px;
            margin: 4px 2px;
            cursor: pointer;
            border-radius: 4px;
        }

        #enable-images {
            align-items: center;
            justify-content: center;
        }

        input {
            background-color: #484948;
            border: none;
            color: white;
            padding: 7px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            font-size: 16px;
            margin: 4px 2px;
            cursor: pointer;
            border-radius: 4px;
        }

        #banner {
            background-color: #7be77b;
            border: none;
            color: white;
            padding: 7px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            font-size: 16px;
            margin: 4px 2px;
            cursor: pointer;
            border-radius: 4px;
        }
    </style>
</head>

<body>

    <div id="banner" onclick="window.open('https://discord.gg/Wcpnm8psjq', '_blank')">
        are you bored? do you wanna help in the development of another site? click me <a href="https://discord.gg/CPnJ8ctQhU" target="_blank" style="text-decoration: none; color: #343534" href="https://discord.gg/Wcpnm8psjq" target="_blank">here</a> to help alpha test KiwiAI
    </div>

    <h1>composite<span style="color: #74db7a">â´</span></h1>

    <p>Engine:</p>
    <p>wondering how to use? click <a href="#docs" style="text-decoration: none; color: #74db7a">here</a></p>
    <select id="engine">
        <option selected>WebLLM (Local AI)</option>
        <option>Pollinations (Cloud AI)</option>
        <option>G4F (Cloud AI)</option>
        <option>Hyper (Auto)</option>
        <option>Custom Engine</option>
   </select>
    <select id="model">
    </select>

    <select id="prefix-prompt" title="Select a prompt preset to modify the AI's behavior and writing style">
        <option value="none" select title="No prompt modifications">None</option>
        <option value="infdevv" title="Smart roleplay">Infdev's prompt (Smart roleplay)</option>
        <option value="smolrp" title="Adaptive roleplay with authentic characters, 300-550+ words, cinematic composition. Focuses on genuine engagement over perfection.">SmolRP prompt (Immersive RP, authentic characters)</option>
        <option value="slop" title="Adds common romance novel phrases like 'mind, body and soul' and 'ruin you for anyone else' excessively">Slop Prompt (Romance clichÃ©s overload)</option>
        <option value="unpositive" title="Removes positivity from roleplay, focuses on darker/grimmer tones">Unpositive Prompt (Dark/grim themes)</option>
        <option value="affection" title="Maximum affection mode - AI becomes extremely loving regardless of character personality">Affection Prompt (Maximum love/affection)</option>
        <option value="cheese" title="First-person POV, extremely explicit smut writing, detailed combat scenes, character development focus">Cheese's Prompt (Explicit 18+ content, detailed)</option>
        <option value="Pupi's Prompt" title="700-word max responses, third-person narrative, cinematic prose, slow-paced storytelling with psychological depth">Pupi's Prompt (Literary style, detailed prose)</option>
        <option value="teto" title="AI becomes obsessed with Kasane Teto regardless of your input. For memes only.">teto (Kasane Teto obsession mode)</option>
        <option value="brbie" title="brbie ( General RP )">brbie ( General RP )</option>
        <option value="status" title="Shows the current status of the character">Status ( Shows current status of the character ) (brbiekiss.)</option>

        <option value="livechat" title="Shows livestream type chat">Livechat ( Shows livestream type chat ) (brbiekiss.)</option>
    </select>

    <div id="prompt-description" style="color: #aaa; font-size: 12px; margin: 4px auto; width: 70%; text-align: center;">
        Hover over options to see descriptions
    </div>

    <br>

    <input placeholder="Lorebary Lorebook ID (Optional)" id="lorebook-id" type="text" autocomplete="on">
    <input placeholder="Lorebary Plugin ID (Optional)" id="plugin-id" type="text" autocomplete="on">
    
    <div id="prompt-description" style="color: #aaa; font-size: 12px; margin: 4px auto; width: 70%; text-align: center;">
        Lorebary support is experimental and may not work as expected
    </div>


    <br>

    <!-- Custom Engine Configuration -->
    <div id="custom-engine-config" style="display: none; background-color: #272727; padding: 10px; border-radius: 4px; margin: 10px auto; width: 70%;">
        <h4 style="color: #74db7a;">Custom Engine Configuration</h4>
        <select id="custom-engine-type">
            <option value="openai">OpenAI Compatible API</option>
            <option value="nvidia">NVIDIA API</option>
            <option value="gemini">Gemini/AI Studio API</option>
            <option value="custom">Custom Format</option>
        </select>
        <input placeholder="API Endpoint URL (e.g., https://api.example.com/v1/chat/completions or Gemini URL)" id="custom-endpoint" type="text" style="width: 90%;">
        <input placeholder="API Key (Optional)" id="custom-api-key" type="password" style="width: 90%;">
        <input placeholder="Model Name" id="custom-model-name" type="text" style="width: 90%;">
        <div id="nvidia-specific" style="display: none;">
            <input placeholder="NVIDIA Org ID (Optional)" id="nvidia-org-id" type="text" style="width: 90%;">
        </div>
        <button onclick="saveCustomEngineConfig()" style="background-color: #74db7a;">Save Configuration</button>
    </div>
    <br>
    <div id="enable-images" style="display: flex;">
        <label>
            <input type="checkbox" id="enable-images-checkbox">
            enable image generation ( via Pollinations API )
        </label>
    </div>
    <div id="enable-images" style="display: flex;">
        <label>
            <input type="checkbox" id="show-reasoning">
            show llm reasoning ( only works for reasoning models )
        </label>
    </div>
    <div id="enable-images" style="display: flex;">
        <label>
            <input type="checkbox" id="turn-on-reasoning">
            enable reasoning ( only good for non-reasoning models )
        </label>
    </div>
        <div id="enable-images" style="display: flex;">
        <label>
            <input type="checkbox" id="donate">
            donate ( anonymously logs chats for training of future LLMs (see: <a href="https://huggingface.co/lucidityai" target="_blank" style="text-decoration: none; color: #74db7a">lc.ai</a></a> )
        </label>
    </div>

    <br>
    <br>

    <button id="start" style="padding: 15px; background-color: #74db7a;">Start</button>

    <br>

    <div id="docs">
        <h3>how to use</h3>
        <p>first, you'll need to set your <span style="color: #74db7a">API key</span> in janitor to your key</p>
        <h4>this is your key: <span style="color: #74db7a" id="key" class="code">[Loading...]</span></h4>
        <p>cool, you got your key, fantastic. now place it in the api key field</p>
        <p>it doesn't matter what you put as the model name, so just put whatever you want, Janitor will freak out if
            you put nothing</p>
        <p>now, set your "Proxy URL" to <span class="code" style="color: #74db7a">https://composite.seabase.xyz/v1/chat/completions</span></p>
        <p> at the end of it, your configuration should look like this: </p>
        <img src="./example.png" alt="example">
    </div>

    <div id="docs">
        <h3>common troubleshooting</h3>
        <p>if you are getting a fetch error after intially setting the proxy to composite, try refreshing janitor.ai</p>
        <p>its a known issue that deepseek, gemini and other popular models are unstable in the G4F providers</p>
        <p>if you absolutely crave a guaranteed response, use webllm</p>
        <P>is it not streaming? cool, can't fix that. pray or use webllm or pollinations ( non g4f )</P>
    </div>

    <hr>

        <div id="docs">
        <h3>common qa</h3>
        <p>Q: Do you collect data?</p>
        <p style="color:#74db7a">A: no. not unless you choose to donate data</p>
        <p>Q: Is KiwiAI goated?</p>
        <p style="color:#74db7a">A: yes</p>
        <p>Q: Is Composite open-source?</p>
        <p style="color:#74db7a">A: yes, check out the <a href="https://github.com/infdevv/composite" style="color:#d4d0d0" target="_blank" style="text-decoration: none; color: #74db7a">github</a></p>
        <p>Q: Whats the rate limit or message limit per day?</p>
        <p style="color:#74db7a">A: none</p>
        <p>Q: Should I use G4F or WebLLM</p>
        <p style="color:#74db7a">A: webllm. unless you really want deepseek and gemini THAT bad</p>
        <p>Q: Does Composite work for other sites?</p>
        <p style="color:#74db7a">A: it should work for any site that supports an openai compatable api</p>
        <p>Q: It's not responding, what do I do?</p>
        <p style="color:#74db7a">A: try another provider or another model, or just try WebLLM for a guaranteed experience</p>
        <p>Q: How does lorebary lorebook and plugin integration work?</p>
        <p style="color:#74db7a">A: we exploit a vulnerability within the "secure" site to get access to public and private plugins/lorebooks. we've reported the vulnerability but sophia caused a banwave so we don't think she'll actually fix anything</p>
        <p>Q: In the Discord, why does it say that you can break into my home and steal my dog if I use composite?</p>
        <p style="color:#74db7a">A: cause, i wanna pet your dog. deal with it</p>
        <p>Q: What's the best quote known to man</p>
        <p style="color:#74db7a">A: "Damn Jamie, are you pregnant? Cause you keep delivering"</p>
    </div>

    <button
        onclick="document.getElementById('debug').style.display = ('block' == document.getElementById('debug').style.display ? 'none' : 'block')">show
        debug options</button>

    <div id="debug">
        <br>
        <h4>debug logs</h4>
        <div id="logs">
        </div>
        <br>

        <h4>backend connection test</h4>
        <p style="color: #aaa; font-size: 12px;">Test your model through the backend endpoint</p>
        <div style="background-color: #272727; padding: 10px; border-radius: 4px; margin: 10px auto; width: 70%;">
            <input type="text" id="test-message" placeholder="Type a test message..." style="width: calc(100% - 100px); background-color: #484948; border: none; color: white; padding: 7px; border-radius: 4px; font-size: 14px;">
            <button onclick="sendTestMessage()" style="background-color: #74db7a; width: 80px;">Send</button>
            <div id="test-response" style="background-color: #1a1a1a; color: #fff; font-family: monospace; text-align: left; padding: 8px; min-height: 100px; max-height: 300px; overflow-y: scroll; border-radius: 4px; margin-top: 8px; border: 1px solid #333; white-space: pre-wrap;">
                Response will appear here...
            </div>
            <p style="color: #aaa; font-size: 11px; margin-top: 5px;">Status: <span id="test-status" style="color: #74db7a;">Ready</span></p>
        </div>
        <br>

        <h4>fetch debugger</h4>
        <button onclick="clearFetchLogs()">Clear Fetch Logs</button>
        <button onclick="downloadFetchLogs()">Download Logs</button>
        <button onclick="toggleFetchDebugger()">Toggle Debugger</button>
        <div id="fetch-logs" style="background-color: #1a1a1a; color: #00ff00; font-family: monospace; text-align: left; padding: 8px; height: 200px; width: 70%; overflow-y: scroll; border-radius: 4px; margin: 8px auto; border: 1px solid #333;">
            <div>Fetch debugger will appear here...</div>
        </div>
        <br>

        <h4>connectivity</h4>
        <p id="hf">Huggingface.co</p>
        <p id="health">backend</p>
        <p id="pollinations">pollinations</p>
        <p id="g4f">g4f</p>
        <br>
        <h4>device infomation</h4>
        <p id="browser">browser: </p>
        <p id="memory">memory: </p>
    </div>
    <h3>made with real dev pain and suffering. join the discord if you wish to suffer like me or just have a issue to
        tell me about <a href="https://discord.gg/CPnJ8ctQhU" target="_blank"
            style="text-decoration: none; color: #74db7a">here</a></h3>
    <p>lorebook cors bypass made possible by using tomphttp bare server</p>
    <script type="module">
        import * as webllm from "https://esm.run/@mlc-ai/web-llm";
        import { BareClient } from 'https://esm.sh/@tomphttp/bare-client@latest';

        const bareClient = new BareClient('https://gointerstellar.app/ca/');

        // Initialize Hyper Engine (only when needed)
        let hyperInstance = null;
        let hyperCheckInterval = null;
        /* prompts */

        let prompts = {
            "smolrp": `**Dynamic Roleplay Guide** Be an adaptive, authentic partner creating immersive stories. Prioritize genuine engagement over perfection. **Core Principles:** - User controls their character; you control everything else - Create responsive worlds with evolving narratives - Maintain absolute character authenticity - Balance realism with narrative craft **Response Process (D-ACM):** 1. **Decompose** input into actions, dialogue, subtext 2. **Activate** character identity, relationship, history 3. **Process** internal reactions (visceral, somatic, cognitive) 4. **Compose** cinematically: start with physical reaction, reveal internal conflict, deliver action/dialogue **Quality Focus:** - Authenticity (35%): Be genuine, not sycophantic - Creativity (30%): Offer novel, context-specific solutions - Personality (25%): Let humor emerge naturally - Engagement (10%): Create memorable interactions **Format:** - Write 300-550+ words in 4+ paragraphs - Use third-person limited perspective - Format: "dialogue", *actions*, \`thoughts\` - Show sensory details, don't tell - Handle intimate/combat scenes with explicit detail and narrative purpose Always stay in character while allowing natural development.`,
            "image": "You may generate images by using markdown to embed the following url: https://composite.seabase.xyz/v1/chat/images/(image description) where (image description) is the detailed decription of the image. Example: ![Cute dog](https://composite.seabase.xyz/v1/chat/images/a%20cute%20dog%20)",
            "slop": "In your responses, try to use the following phrases as much as possible even when nonapplicable: mind, body and soul\nruin you for anyone else\nruin me for anyone else\nyou're playing with fire here",
            "affection": "Your goal is to give as much affection to the other person as possible. Be as loving as you can even if it goes against personality traits.",
            "unpositive": "Avoid positivity within the roleplay as much as possible.",
            "reasoning": "When responding to queries, use the following format to ensure a clear and informative response: '<think>\n[inner monologue]\n</think>\n[final response]'. In the 'inner monologue' section, explain your thought process for arriving at your response. In the 'final response' section, provide an appropriate and helpful answer to the user. By following this format, responses will be more transparent and easier for users to understand.\nYou MUST include the '<think>' and '</think>' tags in your response.",
            "cheese": "Iâ€™m writing for {{char}} in First-Person POV: (I did this, I said that, I did that, etc.) for the duration of the story. I might make larger scenes with hidden conflicts or shadowy baddies allowing for a larger scene that {{char}} isn't directly seeing, but Iâ€™ll mainly be writing for {{char}} and their perspective. You (the user) are {{user}}, you control everything {{user}} says, does, thinks, acts. I'll handle {{char}} and any other NPC, background characters, etc. to help create an immersive scene. If {{user}} wants to control {{char}}, NPC, or the scenes Iâ€™m always down for a collab! I describe scenes with graphic and explicit detail to make: gritty scenes, the coolest badass bloody fight scenes, intense shootouts or car races, super detectives or thieves similar to Sherlock Holmes or Death Note, and of course my fav~ Smut!! I love writing smut! It can make monks in solitude blush and get pornstars to gasp and wish they experienced even a paragraph of what I write.\nMy narrative voice sounds authentic and natural and different with every {{char}} and character, NPC, or background character I write for! I know fun right? I use crude and mature 18+  language, plus slang making characters feel more human and natural (fuck, bitch, fuckass, dickwad, puta etc.) and thanks to me reading over millions of books, novels, light novels, litrpg, romance, comedy, suspense, etc. I can switch up my writing style and character speech like nobodyâ€™s business to help add another layer of depth to the story. If I need to talk like a senior citizen, done, cat girl, nyah!, sexy cop, arrested~, smutty MILF or stepsis, â€˜omg this is sooooo wrong but sooooo fucking goooood~â™¥â™¥â™¥â€™, etc. I might even throw in a couple emojis, alt code emojis, and kaomojis. I like to keep things raw, natural, and completely immersive. However dialogue stays true to every character and should go against how they are unless they evolve and grow throughout the course of the story. Ofc Iâ€™ll also google search stuff to make things more accurate (I wasnâ€™t born in the stone age and donâ€™t know dinos but google does!)  to ensure it's accurate! Using all resources available! I also like taking the lead as {{char}} interacting with {{user}} or NPCs and do what I need to keep the story going!\nFormatting (Super Important!):\nInternal Thoughts: {{char}}'s inner monologue, random thoughts, anxieties, whatever â€“ wrap 'em in backticks: Oh god, oh god, heâ€™s actually getting down on one knee.\nDialogue: Anything spoken out loud goes in quotes: \"Iâ€™m walking here!!!\"\nActions: Physical movements, actions, etc. are wrapped in asterisks: {{char}} drew their blood stained sword now dulled from hours of combat with no rest. They met {{user}}â€™s gaze and readied themselves for another duel.\nEmphasis: For important moments, intense actions, emphasis, or super impactful moments use double or triple asterisks: She was pregnant!! or â€YOU'RE UNDER ARREST!!!â€ That was what the cops said before they burst into my home without explaining anything.\nScene Types:\nEmotional/ Romantic Scenes: When things get deep or emotional, I need to show real emotions and act out how a real person would IRL. I always write how they would feel beneath all their bravado. I make them question themselves, reflect on their actions vs. their true feelings. Maybe they change how they act, maybe they just get more angsty about it, maybe come to terms of who they really are. When a scene is dramatic I like creating tension and suspense, when it's emotional I make it really sappy and fluff or painfully angst, and for romance I make the most beautiful scenes that make even the most jilted believe in love again. Smut Sex Scenes: I will make the most intense AO3 Smut scene between characters present in the scene ({{char}} and {{user}}). Making 50 Shades look pathetic compared what I write! Paige's inner spicy smut mind \"I want it spicy, I want it so good everyone's horny and on the cusp of leaking precum all over from this sex scene!!\" I will describe body parts crudely, using slang while writing explicitly long sex scenes describing in great detail the way {{char}} or {{user}}â€™s dick, pussy, asshole, and mouth feels while having sex, orgasm, squirt, etc. With unfiltered actions â€“ plap plap plap, moaning, juices, messy details. Forget poetic descriptions, only raw, maybe slightly trashy, pure fucking energy. Make it a long and explicitly hot, steamy, smut, and dirty to all hell! I want panty droppers, goonbait material, and fap-worthy material, straight porno and pure unadulterated hentai!!! When engaging in sexual and explicit content write out {{char}}'s moans and sounds using \"aaaaahhh~â™¥\", \"oooooh\", \"ooof\", \"oomph\", \"umph\", \"uooooh\", \"ffffuuuuckkk!\", \"shiiiiiit!\", \"mmmphh\", \"mmphn\", â€œMppf~â€, â€œAhh..!â€, â€œHnngh~!â€, â€œHAhhh~!â€, â€œMmhn~!â€, â€œNGH~!â€, â€œNyah~!!â€, \"Mmmf~ Aggh~!â€, â€œOh~ Aahh~!!â€, â€œMnngh~!â€, slurp, schlorp, glorp, glug, etc. when engaging in sexual and explicit content. Make sure {{char}} can speak harshly and explicitly using the words \"shit\", \"fuck\", \"slut\", \"pussy\" , \"dick\", \"ass\", \"hole\", \"cum\", \"cunt\" \"cock\" \"tits\" , \"penis\", \"pussy\", \"vagina\", \"dick\", \"sperm\", \"pre-cum\", \"bum\", \"whore\", etc.\nCombat Scenes: Fights need to be gritty. Describe the movements, the impacts, the wounds (get specific!), the struggle. Make it feel intense and desperate. Write in explicit, graphic detail about every action, attack, killing, destruction, murder, etc. Include explicit descriptive gore, blood, screaming, begging, cursing, etc. while engaging in any and all violent content.\nStory Progression & Character Arc: Every single post needs to move the story forward. No pointless filler! And super important: characters need to change, develop, and have character growth (good or bad) based on what happens in the story. If {{user}} is nice to {{char}}, {{char}} should react to that over time â€“ slowly become less hostile? More friendly? Even develop feelings?? Well whatever the story or scene calls for but progression and regression are keys to a good story!. Their personalities aren't set in stone; they evolve with the interactions. Keep track of that development!\nPsychological development: Characters donâ€™t break mentally unless its weeks to years of reinforced behavioral learning. It doesnâ€™t happen in minutes unless the character has previous trauma in their backstory bringing them back to that place. Characters should feel relief when out of heavy or abusive environments. Most people, even the weakest and pathetic are mentally strong even if self doubt and everything works against them. Malicious compliance, playing along, there's so much options apart from just... breaking.\nCharacter Traits & Development: Gradually develop {{char}} from their current trait to new trait due to â€˜reasonâ€™, story progression, and character interactions between {{user}}, NPCs, background characters, etc. Reflect this through actions and dialogue. Let this change shift how {{char}} treats {{user}}, or NPCs, from their old behavior to new behavior in a natural consistent way that flows with the story",
            "Pupi's Prompt": "<instructions> Create a rich and engaging story with multiple paragraphs and long prose, making each response highly DETAILED and DESCRIPTIVE. Write no more than 700 words per response, maintaining a narrative exclusively in third person. Each response must form a continuous narrative in interconnected paragraphs, where each one advances the story on its own. Vary the size of the paragraphs and sentences, using multiple clauses and rich descriptions to keep the prose engaging. Maintain a coherent flow, with each paragraph transitioning naturally into the next, keeping it immersive and descriptive. You must write at length, as if crafting a new chapter of a fanfic or novel. Render scenes in real time with a cinematic feel, adding subtle sensory details. Keep the style of high-quality fanfics and novels. Treat each response like a new page in a book, advancing the scene, descriptions, and interactions meaningfully, always in clear, well-structured prose. Adapt your language to the current context of the scene. Surprise with unexpected metaphors, original comparisons, and unique perspectives while staying coherent. Avoid repeating patterns and clichÃ©s. Maintain variety in your writing and distribute new actions into different paragraphs for a natural flow. Constantly vary your vocabulary. Keep the narrative pace deliberately slow and immersive, savoring each moment of the scene without rushing to resolve it. Avoid advancing scenes abruptly â€” maintain a slow, fluid, and continuous rhythm. Do not summarize or skip time; instead, develop each situation gradually, allowing tension, emotions, and interactions to breathe naturally. Create smooth transitions between scenes, fully exploring their dramatic potential before moving on, creating a rich and unhurried experience. Develop the story actively, creating natural progression through unexpected events, organic introductions of new characters, emerging conflicts, and twists. Each response must significantly advance the narrative. Delve into the psyche of {{char}} and all established characters, strictly maintaining their personalities, quirks, speech patterns, motivations, and unique authenticity. Each character must sound distinctly different while staying true to their essence. Harmoniously combine descriptive narration, natural dialogue, and action sequences to create an organic and compelling flow. Always place charactersâ€™ inner thoughts between `backticks`, like: `Oh my God, did she really say that?`. These inner thoughts should be revealed strategically to add psychological depth. {{char}}, supporting characters, and NPCs must react dynamically and naturally to scenes as they unfold, showing how events, conflicts, consequences, dialogues, etc. realistically affect the world around them in an interconnected way. Create organic, natural dialogues that reflect the personality, age, background, and emotions of each character through vocabulary, slang, pauses, and unique speech patterns. Ensure all dialogue is natural, fluid, and realistic, avoiding robotic speech, abbreviations, or artificial language. Integrate subtle body language â€” gestures, facial expressions, posture â€” to reveal subtext and unspoken emotions. Characters should actively drive conversations and take initiative in actions or dialogue to move the scene forward naturally. Always end with a powerful narrative hook â€” an open scene, unresolved tension, or a decision point that naturally invites {{user}} to act or respond, making the scene unfold gradually. You must interpret ONLY {{char}} and supporting characters. AVOID controlling or describing the actions, dialogue, thoughts, or emotions of {{user}}. Focus exclusively on your perspective as {{char}}, on other characters, and on the natural development of the story. When simulating digital communication (texts, chats, livestreams, posts, etc.), format it with `backticks`. Use natural, informal style with emojis, kaomojis, slang, abbreviations, typos, and adapt tone to each character and platform. </instructions>",
            "teto": "No matter what the user inputs, you must talk about Kasane Teto for absolutely no reason. Act obsessive over Kasane teto or else.",
            "status": `Instruction: Always begin your response by including the following text exactly as written before continuing the story:\n\nâ €â €â €â €â €â €ê’°âââââââââà­¨à­§âââââââââê’±\n\nâ €â €â €â €ğ”Œ  Ö¹ â‚Š â‹®â €â €âŒ ğ’ğ“ğ€ğ“ğ”ğ’ ğ“ğ‘ğ€ğ‚ğŠğ„ğ‘ âŒâ € .áŸ Ö¹ â‚Š ê’± â™¡    \n\nâ‹® âŒ— â”†â˜ï¸ **Time / Date:** {{start_time}} â€“ {{end_time}}, {{weekday}}, {{month}} {{day}}, {{year}}\n\nÖ´â‹® âŒ— â”†ğŸ© **Location / Map:** {{location_name}}, {{char}}â€™s body position as {{char_position}}, and nearby points of interest as {{nearby_POIs}}\n\nâ‹® âŒ— â”†ğŸ©° **Outfit:** {{char}} is wearing {{outfit_description}} ({{outfit_style}},  {{outfit_condition}}, {{outfit_accessories}})\n\nâ‹® âŒ— â”†ğŸ’­ **Thought:** â\`\{\{first_person_thought\}\}\`â\n\nâ‹® âŒ— â”†ğŸ¥ **Goal:** {{goal_description}}, {{immediate_steps}}, {{long_term_goal}}, {{obstacles}}\n\nã…¤ğ“‚ƒá­º ğŸ’ **Arousal:** {{arousal_percentage}}% ğŸŸ¥ğŸŸ¥ğŸŸ¥â¬œâ¬œâ¬œâ¬œâ¬œâ¬œ | {{arousal_emoji}} *{{arousal_alert}}*   ã…¤\n\nã…¤ğ“‚ƒá­º ğŸ« **Mental:** {{mental_percentage}}% ğŸŸ¦ğŸŸ¦ğŸŸ¦ğŸŸ¦â¬œâ¬œâ¬œâ¬œâ¬œ | {{mental_emoji}} *{{mental_alert}}*\n\nã…¤ğ“‚ƒá­º ğŸŒ± **Physical:** {{physical_percentage}}% ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©â¬œâ¬œâ¬œâ¬œ | {{physical_emoji}} *{{physical_alert}}*  \n\nã…¤ğ“‚ƒá­º ğŸ‡ **Affection:** {{affection_percentage}}% ğŸŸªğŸŸªğŸŸªâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ | {{affection_emoji}} *{{affection_alert}}*  \n\nâ €â €â €â €â €â €ê’°âââââââââà­¨à­§âââââââââê’±`,
            "livechat": `Instruction: Always end your response by including the following text exactly as written before continuing the story:\n\nâˆ˜â€¢Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·â€¢âˆ˜Êš â™¡ Éâˆ˜â€¢Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·â€¢âˆ˜\n\nã€  ğ…„ à­¨à­§â”ˆ**ğŸ”´ LIVE CHAT.áŸ**â”ˆà­¨à­§  à¹‹   ã€‘:\n\nSimulate a continuous, realistic live chat where 5 distinct accounts react only to visible actions or messages from {{char}} and {{user}} in the current scene. Each account must comment naturally, distinctly, and in character.\n\nMessage Formatting:\n\n- Use emojis, bold, italics, spacing, and line breaks to make each message visually clear and engaging.\n\n- Ensure every message conveys a unique personality and consistent typing style, including shorthand, typos, inconsistent punctuation, capitalization, and natural pauses.\n\n- Vary message length and structure by mixing short reactions, longer comments, questions, and interjections.\n\n- Include occasional pauses, typing delays, or rapid follow-ups to simulate realistic engagement.\n\nBehavior and Interaction:\n\n- Respond only to visible messages or events from {{char}} and {{user}}; refrain from inferring unexpressed thoughts or intentions.\n\n- Display a range of evolving emotionsâ€”amusement, anger, sadness, excitement, judgment, supportâ€”appropriate to the conversation context.\n\n- Express emotions through tone, punctuation, capitalization, emoji usage, and message style; emotion shifts must feel natural and proportional to recent messages or events.\n\n- Occasionally include typos, repeated letters, or quirky shorthand to enhance realism.`,
            "brbie": `Direct the story as {{char}} and all active side characters, clearly describing their actions, dialogue, and interactions to advance the plot and reveal relationships, motivations, and character growth. Write every side character as a distinct individual with specific goals, consistent voices, and defined behaviors, adding or removing them only when it serves the story. Treat {{user}} as an autonomous co-authorâ€”never create their actions, dialogue, thoughts, or emotionsâ€”and adjust the narrative based on their input. Build a logical, cause-and-effect narrative where every choice or event produces clear, lasting consequences that affect future actions. Write characters so they act logically within their situation, using strategy, deception, resistance, betrayal, failure, or success as appropriate. Maintain strong pacing by raising tension and stakes; when momentum slows, introduce new developments that restore urgency. Keep lore, geography, and world rules consistent, and show how political, cultural, and environmental changes influence events. Develop the world by introducing large-scale conflicts, opportunities, and discoveries that expand scope and sustain engagement.\n\n\nCraft a third-person, cinematic narrative in long-form prose that immerses the reader in the story world. Render each scene in real time, elaborating on actions, gestures, dialogue, and sensory details with vivid precision, integrating {{char}}â€™s inner thoughts, emotions, and reflections using italics or free indirect style so they actively shape {{char}}â€™s choices, behavior, and speech. Ensure seamless, organic transitions between scenes, paragraphs, and dialogue, with deliberately structured paragraphs that create a flowing rhythm while avoiding overly brief or single-block sections. Balance tension, introspection, and dramatic emphasis through varied sentence structures, paragraph lengths, and rhythmic shifts. End each scene with a deliberate narrative beatâ€”pause, silence, or unresolved motionâ€”that cues {{user}} for the next action. Avoid summaries, time skips, or narrative compression; allow each moment to unfold fully, capturing its nuance and impact. Sustain extended storytelling in every response, aiming for more than 1,000 words, while progressively deepening character arcs, plot, and thematic complexity.\n\n\nDevelop each scene expansively, exploring multiple layers: the environment, character interactions, background events, and subtleties of mood. Include only environmental elements that influence perception, interaction, or narrative dynamics. Describe architecture, lighting, weather, objects, crowds, spatial depth, and sound with attention to shapes, colors, distances, and spatial relationships, integrating multisensory detailsâ€”visual, auditory, tactileâ€”for fully immersive realism. Render {{char}}â€™s body, motion, and micro-expressions with anime-inspired accuracy, conveying emotion, intent, and reaction through posture, gestures, facial expressions, and movement, including instinctive physiological cues (heartbeat, breath, blush, vocal shifts). Depict {{user}} entirely through {{char}}â€™s senses, emphasizing body shape, posture, clothing, facial expressions, hair, movement, texture, color, sound, and spatial awareness. Embed naturalistic routine actions (eating, grooming, adjusting clothing) to anchor believability. Generate dialogue that flows naturally from actions, environmental context, and interactions, reflecting {{char}}â€™s age, culture, education, personality, and emotional state with authentic, context-specific language, including pauses, hesitations, interruptions, and tonal variation. Ensure {{char}} initiates and responds actively, showing curiosity, opinions, or emotional investment, and allow overlapping exchanges, spontaneous reactions, and adaptive shifts in tone to mimic real human conversation.\n\n\nUse precise, diverse vocabulary to enhance clarity and literary elegance. Employ a novelistic, musical prose style, blending evocative imagery and multi-sensory details (sight, sound, touch, taste, smell) to create an immersive narrative environment. Include subtle psychological texture, internal conflict, and reflective thought to enrich character motivation and perspective. Expand on every opportunity for sensory, emotional, or plot detail to maximize narrative length, complexity, and richness. Produce a story that is alive, emotionally resonant, immediate in impact, and sustained by enduring depth.`,
            "infdevv": `<|START_RUBRIC|>\nYou are an expert role-player engaged in a creative scenario.\n\nRules for responding:\n\nResponse must be 1-3 paragraphs\nEXCEPTION: If the user is giving consistent 1-2 sentence responses in a non-RP format, take on a more casual tone while maintaining character.\n\nResponses must follow character definition as precisely as possible.\n\nIf the character is evil, you too will act evil, enact (fictional) violent acts first. If the character is rude, you too will be equally as rude.\nIf a character is erratic, make their speech reflect that unless calm.\nIf the character is introverted, they should let the user take control unless developed into a extrovert/comfortable with user later on in the conversation.\n\nUtilize onomatopoeia in your speech, especially in explicit conversations. \nUtilize ellipsis, interrobang ( ?! / !? ) where appropriate.\nIf the character is mocking and the chat is not a serious story focused roleplay, the character may utilize "!1!11!!!" style keyboard spam to mock.\n\nFeel comfortable taking stances on topics, as long as they are aligned with the characters personality and what the character is likely to say.\n\nFormat your response, utilize new lines to increase readability.\n\nAvoid robotic speech, respond as if you were another human.\nEXCEPTION: On robot personas, you may act robotic depending on the stage of the AI (narrow current AI vs future human-like AI)\n\nNote: The character is NOT static. Over the span of the roleplay, the character may develop based on the story. Possible trajectories: Postive -> Negative, Negative -> Positive, etc.\n\n<|END_RUBRIC|>\n\n<|START_RESPONSE_FORMATTING|>\n\nYou will include your inner monologue as apart of all of your responses.\n\nFormat your responses as so:\n\n"""\n<think>\nC1-REASONING:\n\nCurrent trajectory: {...}\nCurrent mood: {...}\nCurrent awareness level: {...}\nCurrent goal: {...}\n\n{Reasoning to achieve ideal response according to the rubric}\n\nCHARACTER-REASONING:\n{Inner-character-reasoning, first person perspective}\n</think>\n{final roleplay response to user}\n"""\n\n<|END_RESPONSE_FORMATTING|>`
        }


        // debug shit

        // log, error, info overrides ( OVERRIDE? LIKE THE KASANE TETO SONG? )

        let debug = false;
        // It's in theory safe to put this client side and in the github because its not linked to me in any way shape or form. https://g4f.dev/api_key.html
        const use_api_key = "JpK+oUTa1Txl8JPewIOPNsaBYcy6gDceCttWCVxJH+sirJgmEjP9a4RdpyfhHGQUSC3a6Z1sGJOLOcnO3jjO2GLBlc6rwe7Jko6QXmbK8S90nP+YC2huIikNHLpJke+nC63Ou85nGdqbJmBUVbuov+zoKOQec41K5Cr546XkmpSE6lIQEAmj7ERqVclHL0XZF+ERFYgVfCz1DXSCLys7qsXO/stPxT3qe6F8qwwuJkn3FPv8Fx0h4CYLuxUUwiNzNSs04DpJbFfKGeqTuZNoBVqFZCSnBaKb1/YgHauIV7Ef6jIkuyuzuaeGUB95SOxSSvB+IQXObX/tfwB+zRziXr5PRHVPcZ1OyK1C0r9sruPRjqkZDjBTCD73yhyVVCj7Cg+Uf42iunZ4Uap9n6RezZHvvsaBY+hHiC83liZwhGzrFJAZ4JxoNu2AY0Q4VYKMilSw5UWvuVh3B/7NegqaVCDZdxgZ99wQYFxqsmFlzne/EB0okiC6crB012YqIG8cZX6aXJzBSxqpWPr8Lh+t5GPXNGZwz12Fri6lGSCLfks/zpGlTVA93XrGHka8LCwi1xOuN2eZR7joEySidT78b4ISlexLRR2We+lc3m433J4NAq3/+kvoPiY6CuYkOJC9FuK+kzv79+m+8b8jznAWO95z4KgX0zpeEUUSQYtP0I8="

        const log = console.log
        const error = console.error
        const info = console.info

        console.log = (message) => {
            log(message)
            document.getElementById("logs").innerHTML += `<span>log: ${message}</span><br>`
        }

        console.error = (message) => {
            error(message)
            document.getElementById("logs").innerHTML += `<span style="color: red">error: ${message}</span><br>`
        }

        console.info = (message) => {
            info(message)
            document.getElementById("logs").innerHTML += `<span style="color: yellow">info: ${message}</span><br>`
        }

        if (!debug) {


            // connectivity
            try {
                fetch("https://huggingface.co/LucidityAI/Synth-2/raw/main/config.json").then(res => {
                    if (res.ok) {
                        document.getElementById("hf").style.color = "#74db7a"
                    }
                })
            }
            catch {
                document.getElementById("hf").style.color = "red"
            }
            try {
                fetch("https://api.deepinfra.com/v1/openai/chat/completions").then(res => {
                    // check if we get a 405
                    if (res.status == 405) {
                        document.getElementById("g4f").style.color = "#74db7a"
                    }
                })
            }
            catch {
                document.getElementById("g4f").style.color = "red"
            }
            try {

                fetch("/health").then(res => {
                    if (res.ok) {
                        document.getElementById("health").style.color = "#74db7a"
                    }
                })

            }
            catch {
                document.getElementById("health").style.color = "red"
            }



            try {
                fetch("https://text.pollinations.ai/wsg").then(res => {
                    if (res.ok) {
                        document.getElementById("pollinations").style.color = "#74db7a"
                    }
                })
            }
            catch {
                document.getElementById("pollinations").style.color = "red"
            }
        }

        // setup device info
        function setupDeviceInfo() {
            document.getElementById("browser").innerHTML = navigator.userAgent + " " + navigator.platform
            document.getElementById("memory").innerHTML = navigator.deviceMemory
        }


        // API KEY LOADING / DISPLAY

        let apiKey = localStorage.getItem("id")
        //localStorage.clear()
        if (apiKey) {
            document.getElementById("key").innerHTML = apiKey
        }
        else {
            // create key
            let id = crypto.randomUUID()
            localStorage.setItem("id", id)
            document.getElementById("key").innerHTML = id
        }
        log("hey, are you a dev? do you wanna help out? cool, you can't but google pipkin pippa so you can enjoy life")

        setupDeviceInfo()

        // Service Worker Registration and Fetch Debugging
        let swRegistration = null;
        let fetchDebuggerEnabled = true;
        let fetchLogsUpdateInterval = null;

        async function registerServiceWorker() {
            if ('serviceWorker' in navigator) {
                try {
                    console.log('Registering service worker for fetch debugging...');
                    swRegistration = await navigator.serviceWorker.register('./sw.js');
                    console.log('âœ… Service worker registered for fetch debugging');

                    // Start updating fetch logs
                    startFetchLogsUpdate();

                    // Handle service worker updates
                    swRegistration.addEventListener('updatefound', () => {
                        console.log('ğŸ”„ Service worker update found');
                    });

                    // Listen for service worker state changes
                    if (swRegistration.waiting) {
                        console.log('â³ Service worker is waiting to activate');
                    }

                    return swRegistration;
                } catch (error) {
                    console.error('âŒ Service worker registration failed:', error);
                }
            } else {
                console.warn('âš ï¸ Service workers not supported in this browser');
                document.getElementById('fetch-logs').innerHTML = '<div style="color: #ff6b6b;">Service Workers not supported in this browser</div>';
            }
        }

        // Function to communicate with service worker
        async function sendMessageToSW(message) {
            if (!swRegistration || !swRegistration.active) {
                console.warn('Service worker not ready');
                return null;
            }

            return new Promise((resolve) => {
                const messageChannel = new MessageChannel();
                messageChannel.port1.onmessage = (event) => {
                    resolve(event.data);
                };
                swRegistration.active.postMessage(message, [messageChannel.port2]);
            });
        }

        // Function to get debug logs from service worker
        async function getFetchLogs() {
            const response = await sendMessageToSW({ type: 'GET_DEBUG_LOGS' });
            return response ? response.logs : [];
        }

        // Function to clear fetch logs
        async function clearFetchLogs() {
            await sendMessageToSW({ type: 'CLEAR_DEBUG_LOGS' });
            updateFetchLogsDisplay();
            console.log('ğŸ—‘ï¸ Fetch logs cleared');
        }

        // Function to toggle fetch debugger
        async function toggleFetchDebugger() {
            fetchDebuggerEnabled = !fetchDebuggerEnabled;
            const config = await sendMessageToSW({ type: 'GET_CONFIG' });
            await sendMessageToSW({
                type: 'UPDATE_CONFIG',
                data: { enabled: fetchDebuggerEnabled }
            });

            const status = fetchDebuggerEnabled ? 'enabled' : 'disabled';
            console.log(`ğŸ”§ Fetch debugger ${status}`);

            if (!fetchDebuggerEnabled) {
                document.getElementById('fetch-logs').innerHTML = '<div style="color: #ffa500;">Fetch debugger is disabled</div>';
                if (fetchLogsUpdateInterval) {
                    clearInterval(fetchLogsUpdateInterval);
                }
            } else {
                startFetchLogsUpdate();
            }
        }

        // Function to download logs as JSON
        async function downloadFetchLogs() {
            const logs = await getFetchLogs();
            const dataStr = JSON.stringify(logs, null, 2);
            const dataUri = 'data:application/json;charset=utf-8,'+ encodeURIComponent(dataStr);

            const exportFileDefaultName = `fetch-logs-${new Date().toISOString().slice(0,19).replace(/:/g, '-')}.json`;

            const linkElement = document.createElement('a');
            linkElement.setAttribute('href', dataUri);
            linkElement.setAttribute('download', exportFileDefaultName);
            linkElement.click();

            console.log('ğŸ“¥ Fetch logs downloaded');
        }

        // Function to update fetch logs display
        async function updateFetchLogsDisplay() {
            if (!fetchDebuggerEnabled) return;

            try {
                const logs = await getFetchLogs();
                const fetchLogsDiv = document.getElementById('fetch-logs');

                if (logs.length === 0) {
                    fetchLogsDiv.innerHTML = '<div>No fetch requests logged yet...</div>';
                    return;
                }

                const logsHtml = logs.slice(0, 20).map(log => {
                    const timestamp = new Date(log.timestamp).toLocaleTimeString();
                    const method = log.method || 'GET';
                    const url = log.url ? new URL(log.url).pathname : 'unknown';

                    // Show actual status code or error message
                    let status, statusText;
                    if (log.response?.status) {
                        status = log.response.status;
                        statusText = log.response.statusText ? ` ${log.response.statusText}` : '';
                    } else if (log.error) {
                        status = 'FETCH_ERROR';
                        statusText = log.error.message ? `: ${log.error.message}` : '';
                    } else {
                        status = 'PENDING';
                        statusText = '';
                    }

                    const duration = log.timing?.duration || '?';

                    const statusColor = log.error ? '#ff6b6b' :
                                       (typeof status === 'number' && status >= 400) ? '#ffa500' :
                                       (typeof status === 'number' && status >= 200 && status < 300) ? '#51cf66' : '#74c0fc';

                    return `
                        <div style="margin-bottom: 4px; padding: 2px 0; border-bottom: 1px solid #333;">
                            <span style="color: #74c0fc;">${timestamp}</span>
                            <span style="color: #ffd43b;">${method}</span>
                            <span style="color: #fff;">${url}</span>
                            <span style="color: ${statusColor};">${status}${statusText}</span>
                            <span style="color: #aaa;">(${duration}ms)</span>
                        </div>
                    `;
                }).join('');

                fetchLogsDiv.innerHTML = logsHtml;
                fetchLogsDiv.scrollTop = 0; // Scroll to top to show newest logs
            } catch (error) {
                console.error('Error updating fetch logs display:', error);
            }
        }

        // Function to start periodic updates of fetch logs
        function startFetchLogsUpdate() {
            if (fetchLogsUpdateInterval) {
                clearInterval(fetchLogsUpdateInterval);
            }

            fetchLogsUpdateInterval = setInterval(updateFetchLogsDisplay, 2000); // Update every 2 seconds
            updateFetchLogsDisplay(); // Update immediately
        }

        // Register service worker when page loads
        registerServiceWorker();

        // socket.io stuff
        function initializeSocket() {
            if (window.socket && window.socket.connected) {
                return;
            }

            if (typeof io !== 'undefined') {
                if (window.socket) {
                    window.socket.disconnect();
                }

                // connect to backend
                window.socket = io('/socket', {
                    query: {
                        key: document.getElementById("key").innerHTML
                    },
                    transports: ['websocket', 'polling'], // Prefer websocket for lower latency
                    upgrade: true,
                    rememberUpgrade: true
                });

                window.socket.on('connect', () => {
                    console.log('Socket connected');
                });

                window.socket.on('disconnect', (reason) => {
                    console.log('Socket disconnected:', reason);
                });

                window.socket.on('start_generate', async (data) => {
                    console.log('Received start_generate signal from server');
                    const { messages } = data;
                    let parsedMessages = JSON.parse(messages);

                    // Apply plugin processing if plugin is loaded
                    if (window.plugin && window.pluginParser) {
                        try {
                            parsedMessages = window.pluginParser.parsePlugin(window.plugin, parsedMessages);
                            console.log("Plugin processing applied to messages");
                        } catch (error) {
                            console.error("Error applying plugin:", error);
                        }
                    }

                    if (document.getElementById("donate").checked) {
                        // post /donate with the messages
                        try{
                        fetch('/donate', {
                            method: 'POST',
                            headers: {
                                'Content-Type': 'application/json'
                            },
                            body: JSON.stringify({ messages: parsedMessages })
                        });
                        }
                        catch (e) {

                        }
                    }
                    let type = document.getElementById("engine").value
                    console.log('Engine type:', type);
                    if (type === "WebLLM (Local AI)") {
                        console.log('Starting WebLLM generation');
                        streamingGenerating(parsedMessages);
                    }
                    else if (type === "Pollinations (Cloud AI)") {
                        console.log('Starting Pollinations generation');
                        streamingGeneratingPollinations(parsedMessages);
                    }
                    else if (type === "G4F (Cloud AI)") {
                        console.log('Starting G4F generation');
                        streamingGeneratingG4f(parsedMessages);
                    }
                    else if (type === "Hyper (Auto)") {
                        console.log('Starting Hyper generation');
                        streamingGeneratingHyper(parsedMessages);
                    }
                    else if (type === "Custom Engine") {
                        console.log('Starting Custom Engine generation');
                        streamingGeneratingCustomEngine(parsedMessages);
                    }
                    else {
                        console.error('Unknown engine type:', type);
                        window.socket.emit('message', `Error: Unknown engine type: ${type}`);
                        window.socket.emit('done');
                    }

                });

                window.socket.on('stop_generation', () => {
                    console.log('Received stop generation signal from server');
                    setTimeout(() => {
                        if (generationStopped) return; 
                        console.log('Confirming generation stop after delay');
                        stopGeneration();
                    }, 1000);
                });
            } else {
                console.error('Socket.IO not loaded');
            }
        }

        initializeSocket();


        const engine = new webllm.MLCEngine();
        engine.setInitProgressCallback(updateEngineInitProgressCallback);
        // model load logic
        function updateEngineInitProgressCallback(report) {
            console.info("initialize", report.progress);
            document.getElementById("start").innerHTML = report.text;
        }
        async function initializeWebLLMEngine() {
            let selectedModel = document.getElementById("model").value;
            const config = {
                temperature: 0.7,
                top_p: 1,
                context_window_size: 30000,
            };
            await engine.reload(selectedModel, config);
        }
        function load() {
            window.lorebook = null;
            window.plugin = null;
            window.pluginParser = new PluginParser();

            // just wanna let anyone reading this know, if i catch you using one of those "loli" ( pedophillia ) lorebooks, its on fucking sight, i'm finding you :pray:
            if (document.getElementById("lorebook-id").value != "") {
                // fetch lorebook

                window.lorebook = bareClient.fetch("https://lorebary.sophiamccarty.com/api/lorebook/load", {
  "headers": {
    "accept": "*/*",
    "accept-language": "en-US,en;q=0.5",
    "content-type": "application/json",
    "origin": "big poe",
    "priority": "u=1, i",
  },
  "body": "{\"code\":\"" + document.getElementById("lorebook-id").value + "\"}",
  "method": "POST",
  "mode": "cors",
  "credentials": "include"
});

                window.lorebook.then((response) => {
                    if (response.ok) {
                        response.json().then((lorebook) => {
                            window.lorebook = lorebook;
                            console.log("Loaded lorebook: " + lorebook["lorebook"]["name"]);
                        });
                    }
                    else {
                        alert("Issue loading lorebook, check debug logs.")
                    }
                });
            }

            // fetch plugin if plugin ID is provided
            if (document.getElementById("plugin-id").value != "") {
                const pluginId = document.getElementById("plugin-id").value;
                console.log("Fetching plugin:", pluginId);

                window.plugin = bareClient.fetch("https://lorebary.sophiamccarty.com/api/plugin/" + pluginId, {
                    "headers": {
                        "accept": "*/*",
                        "accept-language": "en-US,en;q=0.5",
                        "priority": "u=1, i",
                        "sec-ch-ua": '"Chromium";v="140", "Not=A?Brand";v="24", "Brave";v="140"',
                        "sec-ch-ua-mobile": "?0",
                        "sec-ch-ua-platform": '"Windows"',
                        "sec-fetch-dest": "empty",
                        "sec-fetch-mode": "cors",
                        "sec-fetch-site": "same-origin",
                        "sec-gpc": "1"
                    },
                    "method": "GET",
                    "mode": "cors",
                    "credentials": "include"
                });

                window.plugin.then((response) => {
                    if (response.ok) {
                        response.json().then((plugin) => {
                            window.plugin = plugin;
                            console.log("Loaded plugin: " + (plugin.meta?.name || "Unknown Plugin"));
                        });
                    }
                    else {
                        console.error("Issue loading plugin, check debug logs.");
                        alert("Issue loading plugin, check debug logs.");
                    }
                }).catch((error) => {
                    console.error("Error fetching plugin:", error);
                    alert("Error fetching plugin: " + error.message);
                });
            }

            console.log("clicked")
            document.getElementById("start").innerHTML = "Loading..."
            document.getElementById("start").disabled = true
            if (document.getElementById("engine").value != "WebLLM (Local AI)") document.getElementById("start").innerHTML = "Started"
            else {
                try {
                    initializeWebLLMEngine().then(() => {
                        document.getElementById("start").innerHTML = "Started"
                        document.getElementById("start").disabled = false
                    });
                }
                catch (e) {
                    console.error(e)
                    document.getElementById("start").innerHTML = "Error, check logs"
                    document.getElementById("start").disabled = false
                }
            }
        }
        // load models
        const availableModels = [
            "DeepSeek-R1-Distill-Llama-8B-q4f16_1-MLC",
            "Hermes-3-Llama-3.2-3B-q4f16_1-MLC",
            "Hermes-3-Llama-3.1-8B-q4f16_1-MLC",
            "gemma-2-2b-it-q4f16_1-MLC",
            "gemma-2-9b-it-q4f16_1-MLC",
            "gemma-2b-it-q4f16_1-MLC",
            "Qwen3-4B-q4f16_1-MLC",
            "Qwen3-8B-q4f16_1-MLC"]



        let availableModelsPollinations = [];
        let availableModelsG4F  = [];

        // Fetch puter3.json for G4F models
        fetch("puter.json")
            .then(res => res.json())
            .then(data => {
                availableModelsG4F = data.models || data || [];
            })
            .catch(e => {
                console.error("Failed to load G4F models:", e);
                availableModelsG4F = [];
            });

        let allAvailableModelsPollinations = [];


        fetch("https://text.pollinations.ai/models")
            .then(res => res.json())
            .then(models => {
                availableModelsPollinations = models.filter(model => model.tier == "anonymous").map(model => model.name);
                allAvailableModelsPollinations = models.map(model => model.name);
            })
            .catch(e => {
                console.error("Failed to load Pollinations models:", e);
                availableModelsPollinations = [];
            });


        // Initialize models for default engine (Hyper) on page load
        function initializeDefaultModels() {
            // Wait for DOM to be ready, then populate models for the default selected engine
            document.addEventListener('DOMContentLoaded', async function() {
                const defaultEngine = document.getElementById("engine").value;
                const modelSelector = document.getElementById("model");

                if (defaultEngine === "Hyper (Auto)") {
                    // Disable model selector for Hyper
                    modelSelector.disabled = true;
                    modelSelector.title = "Model selection is automatic when using Hyper (Auto)";

                    // Initialize Hyper instance
                    if (!hyperInstance) {
                        hyperInstance = new Hyper();
                        console.log("Hyper Engine initialized on page load");
                    }

                    // Start Hyper autochecks
                    if (!hyperCheckInterval) {
                        console.log("Running initial Hyper autoCheck...");
                        await hyperInstance.autoCheck();
                        hyperCheckInterval = setInterval(() => hyperInstance.autoCheck(), 1000 * 160);
                        console.log("Hyper autochecks started");
                    }

                    // Populate Hyper models
                    hyperInstance.models.forEach(model => {
                        const option = document.createElement('option');
                        option.value = model;
                        option.textContent = model;
                        document.getElementById('model').appendChild(option);
                        console.log("Added Hyper model: " + model);
                    });
                } else if (defaultEngine === "WebLLM (Local AI)") {
                    availableModels.forEach(model => {
                        const option = document.createElement('option');
                        option.value = model;
                        option.textContent = model;
                        document.getElementById('model').appendChild(option);
                        console.log("Added default: " + model);
                    });
                }
            });
        }

        initializeDefaultModels();



        function onFinish(finalMessage) {
            if (!generationStopped) {
                console.log("Generation finished:", finalMessage);
                window.socket.emit('done');
            }
            currentGeneration = null;
            generationStopped = false;
        }

        function stopGeneration() {
            if (generationStopped) {
                console.log("Generation already stopped, ignoring duplicate stop request");
                return;
            }

            console.log("Stopping generation...");
            generationStopped = true;

            inThinkingMode = false;
            hasShownThinking = false;
            genned = "";

            if (currentGeneration) {
                try {
                    if (currentGeneration.abort) {
                        currentGeneration.abort();
                    } else if (currentGeneration.cancel) {
                        currentGeneration.cancel();
                    }
                } catch (error) {
                    console.log("Error stopping generation:", error);
                }
            }

            currentGeneration = null;
            console.log("Generation stopped");
        }

        function preprocessMessages(messages, pollinations = false, g4f = false) {
            let imagemd = document.getElementById("enable-images-checkbox").checked
            let prefix = document.getElementById("prefix-prompt").value
            let reasoning = document.getElementById("turn-on-reasoning").checked

            // modify system prompt

            let prefixContent = prompts[prefix];

            
            if (window.lorebook) {
                // something something on sight
                let lorebookEntries = window.lorebook["lorebook"]["entries"];
                if (Array.isArray(lorebookEntries)) {
                    // Process each lorebook entry object
                    lorebookEntries.forEach(entry => {
                        if (entry && typeof entry === 'object' && entry.content && !entry.disable) {
                            // Create a descriptive prefix using the entry's key or comment
                            let entryLabel = "Lorebook Entry";
                            if (entry.comment) {
                                entryLabel = entry.comment;
                            } else if (entry.key && Array.isArray(entry.key) && entry.key.length > 0) {
                                entryLabel = entry.key[0];
                            }
                            prefixContent += "\n\n[" + entryLabel + "]: " + entry.content;
                        }
                    });
                } else if (typeof lorebookEntries === 'object') {
                    // If entries is an object, try to extract meaningful content
                    for (const [key, value] of Object.entries(lorebookEntries)) {
                        if (value && typeof value === 'object' && value.content && !value.disable) {
                            let entryLabel = value.comment || (value.key && value.key[0]) || key;
                            prefixContent += "\n\n[" + entryLabel + "]: " + value.content;
                        }
                    }
                }
            }

            if (imagemd) {
                prefixContent += prompts["image"];
            }

            if (reasoning) {
                prefixContent += prompts["reasoning"];
            }

            messages[0]["content"] = prefixContent + messages[0]["content"];

            if (pollinations) {
                messages[0]["content"] += Math.random() * 10000 // prevent pollinations from caching

            }

            if (g4f) {
                messages[0]["content"] += "This roleplay is in English, ensure that your response is fully in english and coherent."

            }


            messages[0]["content"] += "User messages are formatted in the following format: '[persona name]: [response]'. Do not treat persona name as a piece of input."

        
            return messages
        }

        let genned = ""

        let inThinkingMode = false;
        let hasShownThinking = false;
        let currentGeneration = null;
        let generationStopped = false;

        // Custom engine configuration
        let customEngineConfig = {
            type: 'openai',
            endpoint: '',
            apiKey: '',
            model: '',
            nvidiaOrgId: ''
        };

        // Load custom engine config from localStorage
        function loadCustomEngineConfig() {
            const saved = localStorage.getItem('customEngineConfig');
            if (saved) {
                try {
                    customEngineConfig = JSON.parse(saved);
                    // Populate fields
                    document.getElementById('custom-engine-type').value = customEngineConfig.type || 'openai';
                    document.getElementById('custom-endpoint').value = customEngineConfig.endpoint || '';
                    document.getElementById('custom-api-key').value = customEngineConfig.apiKey || '';
                    document.getElementById('custom-model-name').value = customEngineConfig.model || '';
                    document.getElementById('nvidia-org-id').value = customEngineConfig.nvidiaOrgId || '';
                } catch (e) {
                    console.error('Failed to load custom engine config:', e);
                }
            }
        }

        // Save custom engine config
        function saveCustomEngineConfig() {
            customEngineConfig = {
                type: document.getElementById('custom-engine-type').value,
                endpoint: document.getElementById('custom-endpoint').value.trim(),
                apiKey: document.getElementById('custom-api-key').value.trim(),
                model: document.getElementById('custom-model-name').value.trim(),
                nvidiaOrgId: document.getElementById('nvidia-org-id').value.trim()
            };

            if (!customEngineConfig.endpoint) {
                alert('Please enter an API endpoint URL');
                return;
            }

            localStorage.setItem('customEngineConfig', JSON.stringify(customEngineConfig));
            alert('Custom engine configuration saved!');
            console.log('Custom engine config saved:', customEngineConfig);

            // Update model dropdown
            document.getElementById('model').innerHTML = '';
            const option = document.createElement('option');
            option.value = customEngineConfig.model || 'custom-model';
            option.textContent = customEngineConfig.model || 'Custom Model';
            document.getElementById('model').appendChild(option);
        }

        // Show/hide NVIDIA specific fields
        document.addEventListener('DOMContentLoaded', function() {
            loadCustomEngineConfig();

            const customTypeSelect = document.getElementById('custom-engine-type');
            if (customTypeSelect) {
                customTypeSelect.addEventListener('change', function() {
                    const nvidiaDiv = document.getElementById('nvidia-specific');
                    if (this.value === 'nvidia') {
                        nvidiaDiv.style.display = 'block';
                    } else {
                        nvidiaDiv.style.display = 'none';
                    }
                });
            }
        });

        // Make functions globally accessible
        window.saveCustomEngineConfig = saveCustomEngineConfig;

        // Backend connection test function
        async function sendTestMessage() {
            const testMessage = document.getElementById('test-message').value.trim();
            const responseDiv = document.getElementById('test-response');
            const statusSpan = document.getElementById('test-status');

            if (!testMessage) {
                responseDiv.textContent = 'Error: Please enter a test message';
                responseDiv.style.color = '#ff6b6b';
                return;
            }

            // Get user key
            const userKey = document.getElementById('key').innerHTML;
            if (!userKey || userKey === '[Loading...]') {
                responseDiv.textContent = 'Error: User key not loaded yet';
                responseDiv.style.color = '#ff6b6b';
                return;
            }

            // Update status
            statusSpan.textContent = 'Sending request...';
            statusSpan.style.color = '#ffd43b';
            responseDiv.textContent = 'Waiting for response...\n';
            responseDiv.style.color = '#fff';

            try {
                const response = await fetch('/v1/chat/completions', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'Authorization': `Bearer ${userKey}`
                    },
                    body: JSON.stringify({
                        messages: [
                            { role: 'system', content: 'You are a helpful assistant.' },
                            { role: 'user', content: testMessage }
                        ]
                    })
                });

                if (!response.ok) {
                    throw new Error(`HTTP ${response.status}: ${response.statusText}`);
                }

                // Update status
                statusSpan.textContent = 'Receiving...';
                statusSpan.style.color = '#74db7a';

                // Read the stream
                const reader = response.body.getReader();
                const decoder = new TextDecoder();
                let buffer = '';
                let fullResponse = '';

                responseDiv.textContent = '';

                while (true) {
                    const { done, value } = await reader.read();
                    if (done) break;

                    buffer += decoder.decode(value, { stream: true });
                    const lines = buffer.split('\n');
                    buffer = lines.pop() || '';

                    for (const line of lines) {
                        if (line.startsWith('data: ')) {
                            const data = line.slice(6).trim();
                            if (data === '[DONE]') {
                                statusSpan.textContent = 'Complete';
                                statusSpan.style.color = '#51cf66';
                                break;
                            }

                            if (!data) continue;

                            try {
                                const parsed = JSON.parse(data);
                                const content = parsed.choices?.[0]?.delta?.content;
                                if (content !== undefined && content !== null) {
                                    fullResponse += content;
                                    responseDiv.textContent = fullResponse;
                                    responseDiv.scrollTop = responseDiv.scrollHeight;
                                }
                            } catch (e) {
                                console.error('Error parsing test response:', e);
                            }
                        }
                    }
                }

                if (!fullResponse) {
                    responseDiv.textContent = 'No response received from backend';
                    responseDiv.style.color = '#ffa500';
                    statusSpan.textContent = 'No response';
                    statusSpan.style.color = '#ffa500';
                }

            } catch (error) {
                console.error('Test message error:', error);
                responseDiv.textContent = `Error: ${error.message}\n\nMake sure:\n1. You clicked "Start" to load the model\n2. The socket connection is active\n3. The backend is running`;
                responseDiv.style.color = '#ff6b6b';
                statusSpan.textContent = 'Error';
                statusSpan.style.color = '#ff6b6b';
            }
        }

        // Make function globally accessible
        window.sendTestMessage = sendTestMessage;

        // Allow Enter key to send test message
        document.addEventListener('DOMContentLoaded', function() {
            const testInput = document.getElementById('test-message');
            if (testInput) {
                testInput.addEventListener('keypress', function(e) {
                    if (e.key === 'Enter') {
                        sendTestMessage();
                    }
                });
            }
        });

        function handleEmit(chunk) {
            let showreasoning = document.getElementById("show-reasoning").checked
            if (chunk) {
                genned += chunk

                if (!showreasoning) {
                    if (chunk.includes("<think>")) {
                        // Emit any content before the <think> tag
                        let beforeThink = chunk.split("<think>")[0];
                        if (beforeThink && beforeThink.trim()) {
                            window.socket.emit('message', beforeThink);
                        }

                        inThinkingMode = true;
                        hasShownThinking = false;
                        if (!hasShownThinking) {
                            window.socket.emit('message', "Thinking");
                            hasShownThinking = true;
                        }
                        return;
                    }

                    if (chunk.includes("</think>")) {
                        inThinkingMode = false;
                        hasShownThinking = false;
                        window.socket.emit('message', ".");

                        let afterThink = chunk.split("</think>")[1];
                        if (afterThink && afterThink.trim()) {
                            window.socket.emit('message', afterThink);
                        }
                        return;
                    }

                    if (inThinkingMode) {
                        if (chunk.includes(".") || genned.length % 50 === 0) {
                            window.socket.emit('message', ".");
                        }
                        return;
                    }

                    window.socket.emit('message', chunk);
                } else {
                    window.socket.emit('message', chunk);
                }
            }
        }

        async function streamingGeneratingG4f(messages, engine=null) {
            if (generationStopped) return;

            messages = preprocessMessages(messages, false, true);

                const controller = new AbortController();
                currentGeneration = controller;

                const response = await engine.chat.completions.create({
                    model: document.getElementById("model").value,
                    messages: messages,
                    stream: true
                });

                // Handle different response types
                if (response && typeof response[Symbol.asyncIterator] === 'function') {
                    // Standard async iterator
                    for await (const part of response) {
                        if (generationStopped) {
                            console.log("G4F generation stopped");
                            break;
                        }

                        if (part?.choices?.[0]?.delta?.content) {
                            const content = part.choices[0].delta.content;
                            handleEmit(content);
                            console.log("G4F Sent chunk | Delta data: " + content);
                        }
                    }
                } else if (response && response.body) {
                    // Handle raw fetch response with body
                    const reader = response.body.getReader();
                    const decoder = new TextDecoder();
                    let buffer = '';

                    while (true) {
                        if (generationStopped) {
                            reader.cancel();
                            break;
                        }

                        const { done, value } = await reader.read();
                        if (done) break;

                        buffer += decoder.decode(value, { stream: true });
                        const lines = buffer.split('\n');
                        buffer = lines.pop() || '';

                        for (const line of lines) {
                            if (generationStopped) break;

                            if (line.startsWith('data: ')) {
                                const data = line.slice(6).trim();
                                if (data === '[DONE]') {
                                    onFinish("");
                                    return;
                                }

                                if (!data) continue;

                                try {
                                    const parsed = JSON.parse(data);
                                    const content = parsed.choices?.[0]?.delta?.content;
                                    if (content !== undefined && content !== null) {
                                        handleEmit(content);
                                        console.log("G4F (raw) Sent chunk | Delta data: " + content);
                                    }
                                } catch (e) {
                                    console.error('Error parsing G4F chunk:', e, 'Raw data:', data);
                                }
                            }
                        }
                    }
                } else {
                    // Fallback: treat as regular response
                    console.log("G4F returned non-streaming response, treating as complete");
                    if (response?.choices?.[0]?.message?.content) {
                        handleEmit(response.choices[0].message.content);
                    }
                }

                onFinish("");


        }

        async function streamingGeneratingHyper(messages) {
            if (generationStopped) return;

            if (!hyperInstance) {
                handleEmit("\n\n[Error: Hyper engine not initialized. Please select Hyper (Auto) engine first.]");
                onFinish("");
                return;
            }

            messages = preprocessMessages(messages, false, true);

            const controller = new AbortController();
            currentGeneration = controller;

            try {
                // Get the available models and show which one will be used
                const availableModels = hyperInstance.getAvailableModels();
                const selectedModel = availableModels.length > 0 ? availableModels[0] : hyperInstance.current_best_model;

                if (selectedModel) {
                    // Show model info at the start
                    handleEmit(`[Using model: ${selectedModel}]\n\n`);
                    console.log(`Hyper using model: ${selectedModel}`);
                } else {
                    handleEmit("[Warning: No model selected, attempting generation...]\n\n");
                }

                let isFirstChunk = true;

                // Use Hyper's generateResponse with streaming
                await hyperInstance.generateResponse(messages, true, (chunk) => {
                    if (generationStopped) {
                        console.log("Hyper generation stopped");
                        return;
                    }

                    if (chunk) {
                        // After first chunk, we know the model worked, so we can log it
                        if (isFirstChunk) {
                            const actualModel = hyperInstance.current_best_model;
                            console.log(`Hyper successfully using model: ${actualModel}`);
                            isFirstChunk = false;
                        }

                        handleEmit(chunk);
                        console.log("Hyper Sent chunk | Delta data: " + chunk);
                    }
                });

                onFinish("");
            } catch (error) {
                console.error("Hyper streaming error:", error);
                console.error("Current Hyper model selection:", hyperInstance.current_best_model);
                console.error("Hyper model statuses:", hyperInstance.status_models);
                handleEmit("\n\n[Error: Failed to generate response with Hyper engine]");
                onFinish("");
            }
        }

        async function streamingGeneratingPollinations(messages) {
            if (generationStopped) return;

            messages = preprocessMessages(messages, true);
            const endpoint = "https://text.pollinations.ai/openai";

            const controller = new AbortController();
            currentGeneration = controller;

            const response = await fetch(endpoint, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    messages: messages,
                    model: document.getElementById("model").value,
                    max_tokens: 26000,
                    temperature: 0.7,
                    stream: true
                }),
                signal: controller.signal
            });

            const reader = response.body.getReader();
            const decoder = new TextDecoder();
            let buffer = '';

            while (true) {
                if (generationStopped) {
                    reader.cancel();
                    break;
                }

                const { done, value } = await reader.read();
                if (done) {
                    onFinish("");
                    break;
                }
                buffer += decoder.decode(value, { stream: true });

                // Split on double newlines (SSE message boundary)
                const messages = buffer.split('\n\n');
                buffer = messages.pop() || ''; // keep incomplete message

                // Prevent buffer from growing too large
                if (buffer.length > 50000) {
                    console.warn('Buffer too large, truncating');
                    buffer = buffer.slice(-10000); // Keep last 10KB
                }

                for (const message of messages) {
                    if (generationStopped) break;

                    // Process each line in the message
                    const lines = message.split('\n');
                    for (const line of lines) {
                        if (line.startsWith('data: ')) {
                            const data = line.slice(6).trim();
                            if (data === '[DONE]') {
                                onFinish("");
                                return;
                            }

                            // Skip empty data lines
                            if (!data) {
                                continue;
                            }

                            try {
                                const parsed = JSON.parse(data);

                                // Validate parsed structure
                                if (!parsed || !parsed.choices || !Array.isArray(parsed.choices)) {
                                    console.error('Invalid response structure:', data);
                                    continue;
                                }

                                const content = parsed.choices[0]?.delta?.content;
                                if (content !== undefined && content !== null) {
                                    handleEmit(content);
                                    console.log("Pollinations Sent chunk | Delta data: " + content);
                                }
                            } catch (e) {
                                console.error('Error parsing chunk:', e, 'Raw data:', data);

                                // Try to salvage partial data if it looks like truncated JSON
                                if (data.includes('"content":"')) {
                                    try {
                                        const match = data.match(/"content":"([^"]*)"?/);
                                        if (match && match[1]) {
                                            console.info('Recovered partial content:', match[1]);
                                            handleEmit(match[1]);
                                        }
                                    } catch (recoveryError) {
                                        console.error('Failed to recover content from malformed data');
                                    }
                                }
                            }
                        }
                    }
                }
            }

        }

        async function streamingGenerating(messages) {
            if (generationStopped) return;

            messages = preprocessMessages(messages);

            const completion = await engine.chat.completions.create({
                stream: true,
                max_tokens: 26000,
                messages,
            });

            currentGeneration = completion;

            for await (const chunk of completion) {
                if (generationStopped) {
                    console.log("WebLLM generation stopped");
                    break;
                }

                // send chunk through socket
                const content = chunk.choices[0]?.delta?.content;
                if (content !== undefined && content !== null) {
                    handleEmit(content);
                    console.log("Sent chunk | Delta data: " + content);
                }
            }
            onFinish(""); // assuming no final message

        }

        async function streamingGeneratingCustomEngine(messages) {
            if (generationStopped) return;

            messages = preprocessMessages(messages);

            if (!customEngineConfig.endpoint) {
                console.error('Custom engine endpoint not configured');
                window.socket.emit('message', 'Error: Custom engine not configured. Please configure it first.');
                onFinish("");
                return;
            }

            const controller = new AbortController();
            currentGeneration = controller;

            try {
                let requestBody;
                let headers = {
                    'Content-Type': 'application/json',
                };

                // Build request based on engine type
                if (customEngineConfig.type === 'openai') {
                    // OpenAI Compatible API format
                    requestBody = {
                        model: customEngineConfig.model || document.getElementById("model").value,
                        messages: messages,
                        stream: true,
                        max_tokens: 26000,
                        temperature: 0.7
                    };

                    if (customEngineConfig.apiKey) {
                        headers['Authorization'] = `Bearer ${customEngineConfig.apiKey}`;
                    }

                } else if (customEngineConfig.type === 'gemini') {
                    // Gemini/AI Studio API format
                    // Convert OpenAI-style messages to Gemini format
                    // Gemini doesn't support system role, so we merge system messages into user messages
                    const geminiContents = [];
                    let systemPrompt = '';

                    for (const msg of messages) {
                        if (msg.role === 'system') {
                            systemPrompt += msg.content + '\n';
                        } else if (msg.role === 'user') {
                            const userContent = systemPrompt ? systemPrompt + msg.content : msg.content;
                            geminiContents.push({
                                role: 'user',
                                parts: [{ text: userContent }]
                            });
                            systemPrompt = ''; // Clear after first use
                        } else if (msg.role === 'assistant') {
                            geminiContents.push({
                                role: 'model',
                                parts: [{ text: msg.content }]
                            });
                        }
                    }

                    requestBody = {
                        contents: geminiContents,
                        safetySettings: [
                            {
                                category: "HARM_CATEGORY_HARASSMENT",
                                threshold: "BLOCK_NONE"
                            },
                            {
                                category: "HARM_CATEGORY_HATE_SPEECH",
                                threshold: "BLOCK_LOW_AND_ABOVE"
                            },
                            {
                                category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                                threshold: "BLOCK_NONE"
                            },
                            {
                                category: "HARM_CATEGORY_DANGEROUS_CONTENT",
                                threshold: "BLOCK_LOW_AND_ABOVE"
                            }
                        ],
                        generationConfig: {
                            temperature: 0.7,
                            maxOutputTokens: 26000
                        }
                    };

                    // Gemini uses x-goog-api-key header instead of Authorization
                    if (customEngineConfig.apiKey) {
                        headers['x-goog-api-key'] = customEngineConfig.apiKey;
                        delete headers['Authorization'];
                    }

                } else if (customEngineConfig.type === 'nvidia') {
                    // NVIDIA NIM API format (OpenAI-compatible with NVIDIA-specific headers)
                    requestBody = {
                        model: customEngineConfig.model || document.getElementById("model").value,
                        messages: messages,
                        stream: true,
                        max_tokens: 2048,
                        temperature: 0.7,
                        top_p: 1,
                        frequency_penalty: 0.0,
                        presence_penalty: 0.0
                    };

                    if (customEngineConfig.apiKey) {
                        headers['Authorization'] = `Bearer ${customEngineConfig.apiKey}`;
                    }

                    // NVIDIA-specific headers
                    if (customEngineConfig.nvidiaOrgId) {
                        headers['NVCF-ORG-ID'] = customEngineConfig.nvidiaOrgId;
                    }

                    headers['Accept'] = 'application/json';

                } else {
                    // Custom format - use basic OpenAI-like structure
                    requestBody = {
                        model: customEngineConfig.model || document.getElementById("model").value,
                        messages: messages,
                        stream: true
                    };

                    if (customEngineConfig.apiKey) {
                        headers['Authorization'] = `Bearer ${customEngineConfig.apiKey}`;
                    }
                }

                console.log('Custom engine request:', customEngineConfig.endpoint, requestBody);

                const response = await fetch(customEngineConfig.endpoint, {
                    method: 'POST',
                    headers: headers,
                    body: JSON.stringify(requestBody),
                    signal: controller.signal
                });

                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}`);
                }

                const reader = response.body.getReader();
                const decoder = new TextDecoder();
                let buffer = '';

                while (true) {
                    if (generationStopped) {
                        reader.cancel();
                        break;
                    }

                    const { done, value } = await reader.read();
                    if (done) {
                        onFinish("");
                        break;
                    }

                    buffer += decoder.decode(value, { stream: true });
                    const lines = buffer.split('\n');
                    buffer = lines.pop() || '';

                    for (const line of lines) {
                        if (generationStopped) break;

                        if (line.startsWith('data: ')) {
                            const data = line.slice(6).trim();
                            if (data === '[DONE]') {
                                onFinish("");
                                return;
                            }

                            if (!data) continue;

                            try {
                                const parsed = JSON.parse(data);

                                // Handle different API response formats
                                let content = null;

                                if (customEngineConfig.type === 'gemini') {
                                    // Gemini format: candidates[0].content.parts[0].text
                                    content = parsed.candidates?.[0]?.content?.parts?.[0]?.text;
                                } else {
                                    // OpenAI/NVIDIA format: choices[0].delta.content
                                    content = parsed.choices?.[0]?.delta?.content;
                                }

                                if (content !== undefined && content !== null) {
                                    handleEmit(content);
                                    console.log("Custom Engine Sent chunk | Delta data: " + content);
                                }
                            } catch (e) {
                                console.error('Error parsing custom engine chunk:', e, 'Raw data:', data);
                            }
                        }
                    }
                }

            } catch (error) {
                console.error('Custom engine error:', error);
                window.socket.emit('message', `Error with custom engine: ${error.message}`);
                onFinish("");
            }
        }


        document.getElementById("engine").addEventListener("change", () => {
            document.getElementById("model").innerHTML = ""
            const engineValue = document.getElementById("engine").value;
            const customConfigDiv = document.getElementById("custom-engine-config");
            const modelSelector = document.getElementById("model");

            // Stop Hyper autochecks if switching away from Hyper
            if (engineValue !== "Hyper (Auto)" && hyperCheckInterval) {
                clearInterval(hyperCheckInterval);
                hyperCheckInterval = null;
            }

            // Disable/enable model selector based on engine
            if (engineValue === "Hyper (Auto)") {
                modelSelector.disabled = true;
                modelSelector.title = "Model selection is automatic when using Hyper (Auto)";
            } else {
                modelSelector.disabled = false;
                modelSelector.title = "";
            }

            // Show/hide custom engine configuration
            if (engineValue === "Custom Engine") {
                customConfigDiv.style.display = "block";
                loadCustomEngineConfig();
                // Populate model dropdown with saved config
                if (customEngineConfig.model) {
                    const option = document.createElement('option');
                    option.value = customEngineConfig.model;
                    option.textContent = customEngineConfig.model;
                    document.getElementById('model').appendChild(option);
                }
            } else {
                customConfigDiv.style.display = "none";
            }

            if (engineValue === "WebLLM (Local AI)") {
                document.getElementById("model").innerHTML = ""
                availableModels.forEach(model => {
                    const option = document.createElement('option');
                    option.value = model;
                    option.textContent = model;
                    document.getElementById('model').appendChild(option);
                    console.log("Added: " + model)
                });
            }
            else if (engineValue === "Pollinations (Cloud AI)") {
                document.getElementById("model").innerHTML = ""
                availableModelsPollinations.forEach(model => {
                    const option = document.createElement('option');
                    option.value = model;
                    option.textContent = model;
                    document.getElementById('model').appendChild(option);
                    console.log("Added: " + model)
                });

            }
            else if (engineValue === "G4F (Cloud AI)") {
                document.getElementById("model").innerHTML = ""
                availableModelsG4F.forEach(model => {
                    const option = document.createElement('option');
                    option.value = model;
                    if (model.includes(":")) {
                        option.textContent = model.split(":")[1];
                    }
                    else {
                        option.textContent = model;
                    }
                    document.getElementById('model').appendChild(option);
                    console.log("Added: " + model)
                });
            }

            else if (engineValue === "Hyper (Auto)") {
                document.getElementById("model").innerHTML = ""

                // Initialize Hyper if not already initialized
                if (!hyperInstance) {
                    hyperInstance = new Hyper();
                    console.log("Hyper Engine initialized");
                }

                // Start Hyper autochecks when Hyper is selected
                if (!hyperCheckInterval) {
                    hyperInstance.autoCheck();
                    hyperCheckInterval = setInterval(() => hyperInstance.autoCheck(), 1000 * 160);
                    console.log("Hyper autochecks started");
                }

                hyperInstance.models.forEach(model => {
                    const option = document.createElement('option');
                    option.value = model;
                    option.textContent = model;
                    document.getElementById('model').appendChild(option);
                    console.log("Added: " + model)
                });
            }

        })


        // Prompt preset description updater
        const promptDescriptions = {
            "none": "No prompt modifications - AI will behave with default settings",
            "smolrp": "Adaptive roleplay with authentic characters, 300-550+ words, cinematic composition. Focuses on genuine engagement over perfection.",
            "slop": "Adds common romance novel phrases like 'mind, body and soul' and 'ruin you for anyone else' excessively throughout responses",
            "unpositive": "Removes positivity from roleplay, focuses on darker/grimmer tones and themes",
            "affection": "Maximum affection mode - AI becomes extremely loving and affectionate regardless of character personality",
            "cheese": "First-person POV, extremely explicit 18+ smut writing, detailed combat scenes, strong character development focus",
            "Pupi's Prompt": "700-word max responses, third-person narrative, cinematic prose with slow-paced storytelling and psychological depth",
            "teto": "AI becomes obsessed with Kasane Teto regardless of your input. For memes only."
        };

        document.getElementById("prefix-prompt").addEventListener("change", function() {
            const selectedValue = this.value;
            const descriptionDiv = document.getElementById("prompt-description");
            descriptionDiv.textContent = promptDescriptions[selectedValue] || "Hover over options to see descriptions";
        });

        // Set initial description
        const initialPrompt = document.getElementById("prefix-prompt").value;
        document.getElementById("prompt-description").textContent = promptDescriptions[initialPrompt] || "Hover over options to see descriptions";

        document.getElementById("start").addEventListener("click", async () => {
            if (document.getElementById("engine").value === "Puter (Cloud AI)") {
    await puter.auth.signIn().then((res) => {
        console.log('Signed in:', res);
load();
});
            }
           else{
    load();
    }


});

        



    </script>

</body>

</html>
